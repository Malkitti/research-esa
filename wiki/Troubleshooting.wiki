#summary Problems that have been experienced with Research-ESA.
#labels Phase-Deploy

= Introduction =

On this page, we list known problems of different versions of Research-ESA. The presented solutions to these problems will hopefully help to avoid the same problems for new installations.

= Research-ESA 2.0 =

== Installing ==

    * === No unique bean of type {{{edu.kit.aifb.ConfigurationManager}}} error ===

This error was caused because the "&"  xml reserved character from the original {{{demo_context.xml}}} example should be replaced by &amp; to produce well formed xml. 

== Building Wikipedia indexes ==  

    * === {{{java.lang.OutOfMemoryError: Java heap space}}} ===

==== Trouble ====

The error is produced when running demo.BuildWikipediaIndex.java in a not-so-powerful hardware. 

==== Shooting ====

Run {{{ demo.BuildWikipediaIndex }}} with java -Xmx1G option. 

    * === NoSuchMethodError in {{{ org.tartarus.snowball.SnowballProgram.stem() }}} ===

Libraries downloaded from the original Snowball stemmer site doesn't work with research-esa. You need the original libraries used in ir_framework_4.0. 

== Calculating ESA vectors and text similarity ==

    * === ESA similarity calculation = 0.0 with non Latin-1 wikipedia databases  ===

==== Trouble ====

When comparing two texts with demo.ComputeESASimilarity, the wikipedia database must be on latin1 charset in order to get a semantic similarity score between the two texts. However, I wanted to work with a full wikipedia database (which in english its supposed to have around 3 million pages), so I created the database via  [http://www.mediawiki.org/wiki/MediaWiki Mediawiki] engine, which doesn't support anymore latin-1 databases. Your only options are an utf-8 binary charset database or a plain utf8 one. I tried to calculate semantic similarity with both, but demo.ComputeESASimilarity always gave ESA = 0.0000. 

==== (Partial) Shooting ==== 

I dumped wikimedia mysql database schema using {{{ mysqldump --nodata }}} to an {{{.sql}}} script and with {{{ sed }}} I changed the charset and collation specifications to latin-1. Then I imported again the [http://dumps.wikimedia.org/enwiki/20110405/ wikipedia dumps] , but mwdumper was not able to import the whole database because of a duplicate index error, which is actually documented in [http://www.mediawiki.org/wiki/Manual:MWDumper mwdumper ] documentation. So now I'm able to calculate ESA similarity in a latin-1 charset database, but with only around 10 percent of wikipedia articles. 

==== Pending questions ====

First I thought that demo.ComputeESASimilarity was calculating 0.00 because it couldn't read the ESA vectors, but when I tried demo.PrintESAVector from the binary utf-8 database I found no problem (output was very similar to the latin-1 database), so I think that the problem might be during the cosinus similarity calculation, but I haven't found anything yet. Any hints? 

==== Update on 03/20/2011 ====

Im still not able to get demo.ComputeESASimilarity different from 0.00 with a full wikipedia database. Now I imported from wikipedia dumps with mwdumper using the --default-character-set=utf8 option on mysql database with binary character set. It worked for on a sample example of 100,000 wikipedia pages, but when I uploaded the 11 million pages of the last full wikipedia dump I got ESA Similarity=0,000000000 again (sigh).